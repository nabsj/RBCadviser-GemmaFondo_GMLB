{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f9a38a9-091d-439b-a489-c3b3ff393b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/baekkwsl/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#허깅페이스 액세스토큰\n",
    "hf_token = \"\"\n",
    "\n",
    "# 허깅페이스 로그인\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6cc8d9-a9f2-4b68-b326-5be5af71f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 13:58:06.430792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-29 13:58:06.438682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 13:58:06.446389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 13:58:06.448655: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 13:58:06.454642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 13:58:07.035733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.9: Fast Gemma2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38235a19-649e-4ca3-8e55-66a8a82f7a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baekkwsl/anaconda3/envs/gmlb/lib/python3.10/site-packages/unsloth/models/_utils.py:866: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n",
      "Unsloth 2024.9 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n"
     ]
    }
   ],
   "source": [
    "## Peft 모델(fine-tunning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\",],\n",
    "                      # \"embed_tokens\", \"lm_head\",]\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3103,\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147d833-7d93-4289-be40-e472d8122dc9",
   "metadata": {},
   "source": [
    "# 데이터셋 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b807e186-2655-4d81-93ff-098025ff267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'url'],\n",
      "    num_rows: 21155\n",
      "})\n",
      "Dataset({\n",
      "    features: ['instruction', 'output', 'url'],\n",
      "    num_rows: 40925\n",
      "})\n",
      "Dataset({\n",
      "    features: ['instruction', 'output', 'url'],\n",
      "    num_rows: 61924\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "alpaca_dataset = load_dataset(\"beomi/KoAlpaca-v1.1a\",  split = \"train\")\n",
    "kin_dataset = load_dataset(\"seungjune/RBC_kin_data\",  split = \"train\")\n",
    "dossa_dataset = load_dataset(\"seungjune/RBC_dossa_data\",  split = \"train\")\n",
    "\n",
    "print(alpaca_dataset)\n",
    "print(kin_dataset)\n",
    "print(dossa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0acf04-bd15-4478-8c67-be5c7de908c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'url'],\n",
      "    num_rows: 124004\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets([alpaca_dataset, kin_dataset, dossa_dataset])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd3d092b-861f-4dc6-a4d3-193b120f2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\"다음은 작업을 설명하는 명령입니다. 요청을 적절하게 완료하는 자세한 응답을 작성하세요.\n",
    "\n",
    "### 지침:\n",
    "{}\n",
    "\n",
    "### 응답:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for instruction, output in zip(examples['instruction'], examples['output']):\n",
    "            text = qa_prompt.format(instruction, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242d5bf9-a822-42d8-9c5e-40858445c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 16,\n",
    "    packing=False,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        max_steps = 2000,\n",
    "        warmup_steps = 10,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 5609,\n",
    "        output_dir = \"checkpoint_gemma2-2b-it\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c02f64-1800-4c75-9649-e2e6ee084dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849b0398-c4a5-437a-8d6a-c82a8f106f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baekkwsl/anaconda3/envs/gmlb/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 124,004 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 2,000\n",
      " \"-____-\"     Number of trainable parameters = 755,957,760\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tper_device_train_batch_size: 4 (from args) != 2 (from trainer_state.json)\n",
      "/home/baekkwsl/anaconda3/envs/gmlb/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "AUTOTUNE bmm(16x2048x256, 16x256x2048)\n",
      "  bmm 0.2509 ms 100.0%\n",
      "  triton_bmm_10 0.3624 ms 69.2%\n",
      "  triton_bmm_14 0.3799 ms 66.0%\n",
      "  triton_bmm_9 0.3871 ms 64.8%\n",
      "  triton_bmm_6 0.3881 ms 64.6%\n",
      "  triton_bmm_13 0.3922 ms 64.0%\n",
      "  triton_bmm_5 0.4649 ms 54.0%\n",
      "  triton_bmm_15 0.4731 ms 53.0%\n",
      "  triton_bmm_2 0.5202 ms 48.2%\n",
      "  triton_bmm_3 0.5396 ms 46.5%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.4317 seconds and 0.0076 seconds precompiling\n",
      "AUTOTUNE bmm(16x2048x2048, 16x2048x256)\n",
      "  bmm 0.2499 ms 100.0%\n",
      "  triton_bmm_33 0.4547 ms 55.0%\n",
      "  triton_bmm_22 0.5171 ms 48.3%\n",
      "  triton_bmm_29 0.5437 ms 46.0%\n",
      "  triton_bmm_32 0.5632 ms 44.4%\n",
      "  triton_bmm_25 0.6129 ms 40.8%\n",
      "  triton_bmm_21 0.6481 ms 38.5%\n",
      "  triton_bmm_37 0.6717 ms 37.2%\n",
      "  triton_bmm_34 0.6840 ms 36.5%\n",
      "  triton_bmm_30 0.6948 ms 36.0%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.5731 seconds and 0.0008 seconds precompiling\n",
      "AUTOTUNE bmm(16x2048x2048, 16x2048x256)\n",
      "  bmm 0.2427 ms 100.0%\n",
      "  triton_bmm_81 0.4526 ms 53.6%\n",
      "  triton_bmm_79 0.5100 ms 47.6%\n",
      "  triton_bmm_76 0.5571 ms 43.6%\n",
      "  triton_bmm_78 0.6005 ms 40.4%\n",
      "  triton_bmm_83 0.6533 ms 37.2%\n",
      "  triton_bmm_89 0.7086 ms 34.3%\n",
      "  triton_bmm_85 0.7158 ms 33.9%\n",
      "  triton_bmm_87 0.7475 ms 32.5%\n",
      "  triton_bmm_92 0.8151 ms 29.8%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.6114 seconds and 0.0006 seconds precompiling\n",
      "AUTOTUNE bmm(16x256x2048, 16x2048x2048)\n",
      "  bmm 0.2427 ms 100.0%\n",
      "  triton_bmm_119 0.4506 ms 53.9%\n",
      "  triton_bmm_117 0.5161 ms 47.0%\n",
      "  triton_bmm_114 0.5530 ms 43.9%\n",
      "  triton_bmm_116 0.6093 ms 39.8%\n",
      "  triton_bmm_121 0.6502 ms 37.3%\n",
      "  triton_bmm_123 0.6881 ms 35.3%\n",
      "  triton_bmm_127 0.7158 ms 33.9%\n",
      "  triton_bmm_125 0.7362 ms 33.0%\n",
      "  triton_bmm_120 0.8008 ms 30.3%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.5647 seconds and 0.0007 seconds precompiling\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 56:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>1.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>0.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>0.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>0.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>0.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>0.731500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>0.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>0.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>0.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>0.777200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>1.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>0.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>0.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>0.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>1.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>0.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>0.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>1.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>1.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>0.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>1.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>0.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>1.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>0.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1067</td>\n",
       "      <td>0.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1073</td>\n",
       "      <td>0.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1074</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>0.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1077</td>\n",
       "      <td>0.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1079</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>0.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1082</td>\n",
       "      <td>1.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>0.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>0.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>0.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1087</td>\n",
       "      <td>0.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>0.760500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>0.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>0.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>0.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>1.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>0.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>0.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>0.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>0.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>1.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1108</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1111</td>\n",
       "      <td>0.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>0.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>0.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>0.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>1.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1118</td>\n",
       "      <td>0.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1119</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>1.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>1.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1123</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1124</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>0.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1129</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1131</td>\n",
       "      <td>0.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1132</td>\n",
       "      <td>1.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>0.424600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>1.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>0.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>0.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>0.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1142</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>0.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>0.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>0.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1151</td>\n",
       "      <td>1.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153</td>\n",
       "      <td>0.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1154</td>\n",
       "      <td>1.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>0.721500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1156</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1157</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1158</td>\n",
       "      <td>1.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1159</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1163</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>0.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>1.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1166</td>\n",
       "      <td>0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167</td>\n",
       "      <td>0.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>0.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171</td>\n",
       "      <td>0.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>1.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>1.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>0.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>0.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178</td>\n",
       "      <td>0.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>0.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1183</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>0.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>0.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1187</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1188</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1191</td>\n",
       "      <td>0.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>0.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1193</td>\n",
       "      <td>0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1194</td>\n",
       "      <td>0.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>0.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>0.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>0.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>0.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1202</td>\n",
       "      <td>0.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1203</td>\n",
       "      <td>0.564300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1204</td>\n",
       "      <td>0.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>0.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.718900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1207</td>\n",
       "      <td>0.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>0.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1209</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1211</td>\n",
       "      <td>0.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1212</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1213</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1214</td>\n",
       "      <td>0.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>0.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>0.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>1.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>0.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>1.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1223</td>\n",
       "      <td>0.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>1.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1226</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1227</td>\n",
       "      <td>1.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1228</td>\n",
       "      <td>0.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1229</td>\n",
       "      <td>0.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>1.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1233</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1234</td>\n",
       "      <td>1.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>0.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>0.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>1.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1251</td>\n",
       "      <td>0.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1252</td>\n",
       "      <td>0.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1253</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1254</td>\n",
       "      <td>0.935100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>0.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1259</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1261</td>\n",
       "      <td>0.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1262</td>\n",
       "      <td>0.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1263</td>\n",
       "      <td>0.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>0.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>0.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1266</td>\n",
       "      <td>0.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>0.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1269</td>\n",
       "      <td>0.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1271</td>\n",
       "      <td>0.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>0.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>0.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>0.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>0.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1281</td>\n",
       "      <td>0.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1282</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1283</td>\n",
       "      <td>0.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>0.781800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1286</td>\n",
       "      <td>1.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287</td>\n",
       "      <td>0.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>0.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1289</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1291</td>\n",
       "      <td>0.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1292</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1293</td>\n",
       "      <td>0.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>0.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>0.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>1.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1298</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1299</td>\n",
       "      <td>0.990900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>0.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1303</td>\n",
       "      <td>0.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>0.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>1.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>0.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1313</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1314</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>0.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1317</td>\n",
       "      <td>0.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1319</td>\n",
       "      <td>0.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1321</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1322</td>\n",
       "      <td>0.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1323</td>\n",
       "      <td>1.355600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1324</td>\n",
       "      <td>1.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1326</td>\n",
       "      <td>0.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>0.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>0.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1329</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1331</td>\n",
       "      <td>0.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1333</td>\n",
       "      <td>0.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1334</td>\n",
       "      <td>0.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>0.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1337</td>\n",
       "      <td>0.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1338</td>\n",
       "      <td>1.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1339</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1341</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1342</td>\n",
       "      <td>0.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1343</td>\n",
       "      <td>0.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>0.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1346</td>\n",
       "      <td>0.726100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1347</td>\n",
       "      <td>0.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1348</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1349</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1351</td>\n",
       "      <td>0.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>0.777400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>0.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1354</td>\n",
       "      <td>0.470100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>0.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1356</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1357</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1358</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1361</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1362</td>\n",
       "      <td>0.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1363</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>1.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1366</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1367</td>\n",
       "      <td>0.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>0.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1371</td>\n",
       "      <td>0.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1372</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1373</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1374</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1377</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>0.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1383</td>\n",
       "      <td>0.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>0.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>0.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1387</td>\n",
       "      <td>0.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1388</td>\n",
       "      <td>0.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389</td>\n",
       "      <td>0.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1391</td>\n",
       "      <td>0.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>0.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1393</td>\n",
       "      <td>1.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1394</td>\n",
       "      <td>1.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1396</td>\n",
       "      <td>0.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1397</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1399</td>\n",
       "      <td>1.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1401</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1402</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1403</td>\n",
       "      <td>1.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>0.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1406</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>0.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1409</td>\n",
       "      <td>0.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1411</td>\n",
       "      <td>0.789300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1412</td>\n",
       "      <td>0.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1414</td>\n",
       "      <td>1.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>0.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>1.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1417</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1418</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1419</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1421</td>\n",
       "      <td>1.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1422</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1423</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>0.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1426</td>\n",
       "      <td>0.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1427</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1429</td>\n",
       "      <td>1.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>0.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>0.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1436</td>\n",
       "      <td>0.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1437</td>\n",
       "      <td>1.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1438</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1439</td>\n",
       "      <td>0.719100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1441</td>\n",
       "      <td>0.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1442</td>\n",
       "      <td>0.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1443</td>\n",
       "      <td>0.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1444</td>\n",
       "      <td>0.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>0.778900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1446</td>\n",
       "      <td>1.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1447</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>0.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1451</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1452</td>\n",
       "      <td>1.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1453</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>0.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1461</td>\n",
       "      <td>0.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1462</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1463</td>\n",
       "      <td>1.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>0.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1466</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1469</td>\n",
       "      <td>0.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1471</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>0.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1473</td>\n",
       "      <td>0.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1474</td>\n",
       "      <td>0.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1477</td>\n",
       "      <td>0.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1478</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1479</td>\n",
       "      <td>0.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.878500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1481</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1482</td>\n",
       "      <td>0.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>0.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1484</td>\n",
       "      <td>0.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1486</td>\n",
       "      <td>0.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1487</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>0.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1489</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1491</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1492</td>\n",
       "      <td>1.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1493</td>\n",
       "      <td>0.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>0.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>0.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1501</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1502</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1503</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>0.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1506</td>\n",
       "      <td>0.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1507</td>\n",
       "      <td>0.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1508</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509</td>\n",
       "      <td>0.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1511</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1513</td>\n",
       "      <td>0.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1514</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515</td>\n",
       "      <td>0.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1516</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1517</td>\n",
       "      <td>1.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1518</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1519</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>0.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1522</td>\n",
       "      <td>1.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1523</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1524</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526</td>\n",
       "      <td>0.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>0.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>0.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>0.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1534</td>\n",
       "      <td>0.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>1.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1537</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1539</td>\n",
       "      <td>0.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1541</td>\n",
       "      <td>0.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1542</td>\n",
       "      <td>0.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1543</td>\n",
       "      <td>1.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>0.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1546</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1547</td>\n",
       "      <td>0.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1548</td>\n",
       "      <td>0.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1549</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1551</td>\n",
       "      <td>0.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>0.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1553</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1554</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1556</td>\n",
       "      <td>1.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>0.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1558</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1559</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1561</td>\n",
       "      <td>1.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562</td>\n",
       "      <td>1.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1563</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>0.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565</td>\n",
       "      <td>0.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1566</td>\n",
       "      <td>0.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1567</td>\n",
       "      <td>1.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>0.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>0.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1572</td>\n",
       "      <td>1.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1573</td>\n",
       "      <td>0.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>0.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>0.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>0.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578</td>\n",
       "      <td>0.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1579</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1581</td>\n",
       "      <td>0.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1582</td>\n",
       "      <td>0.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1583</td>\n",
       "      <td>0.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>0.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1586</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1587</td>\n",
       "      <td>0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1588</td>\n",
       "      <td>1.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1589</td>\n",
       "      <td>0.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1591</td>\n",
       "      <td>1.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593</td>\n",
       "      <td>0.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1594</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>0.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>0.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>0.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1601</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1602</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1603</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1606</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1607</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>0.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1609</td>\n",
       "      <td>1.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>0.968300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>0.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>0.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1617</td>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618</td>\n",
       "      <td>1.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1619</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1621</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1622</td>\n",
       "      <td>1.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1623</td>\n",
       "      <td>0.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>0.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>1.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1627</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1628</td>\n",
       "      <td>0.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1629</td>\n",
       "      <td>1.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1633</td>\n",
       "      <td>0.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1634</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1636</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1637</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1638</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1639</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.552400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1641</td>\n",
       "      <td>0.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>0.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1646</td>\n",
       "      <td>0.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1647</td>\n",
       "      <td>1.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1649</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1651</td>\n",
       "      <td>1.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1652</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1653</td>\n",
       "      <td>0.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1654</td>\n",
       "      <td>0.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>0.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1659</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1662</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663</td>\n",
       "      <td>0.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>1.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>0.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1667</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1668</td>\n",
       "      <td>0.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1669</td>\n",
       "      <td>0.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1671</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>0.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1673</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1676</td>\n",
       "      <td>0.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1678</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1679</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1681</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1682</td>\n",
       "      <td>1.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683</td>\n",
       "      <td>0.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1684</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1686</td>\n",
       "      <td>1.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1687</td>\n",
       "      <td>0.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>0.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1689</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1693</td>\n",
       "      <td>0.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1694</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>0.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1697</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1698</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1699</td>\n",
       "      <td>1.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1701</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1702</td>\n",
       "      <td>0.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1703</td>\n",
       "      <td>0.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705</td>\n",
       "      <td>0.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1706</td>\n",
       "      <td>0.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1707</td>\n",
       "      <td>0.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1708</td>\n",
       "      <td>0.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1709</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1711</td>\n",
       "      <td>0.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1713</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1714</td>\n",
       "      <td>1.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1716</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1717</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1718</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1719</td>\n",
       "      <td>0.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>1.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>1.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>0.801300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1726</td>\n",
       "      <td>0.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1727</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>1.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1729</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1731</td>\n",
       "      <td>0.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1732</td>\n",
       "      <td>1.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1733</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1734</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>1.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1737</td>\n",
       "      <td>1.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739</td>\n",
       "      <td>0.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1741</td>\n",
       "      <td>0.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1742</td>\n",
       "      <td>0.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1743</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1746</td>\n",
       "      <td>0.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1747</td>\n",
       "      <td>0.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1748</td>\n",
       "      <td>0.758600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1751</td>\n",
       "      <td>0.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1753</td>\n",
       "      <td>1.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1754</td>\n",
       "      <td>0.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>0.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1758</td>\n",
       "      <td>0.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>0.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1761</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1762</td>\n",
       "      <td>1.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1763</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1764</td>\n",
       "      <td>0.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765</td>\n",
       "      <td>1.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1767</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>0.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1769</td>\n",
       "      <td>0.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1771</td>\n",
       "      <td>1.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1772</td>\n",
       "      <td>0.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1773</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1774</td>\n",
       "      <td>0.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>0.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1777</td>\n",
       "      <td>0.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1778</td>\n",
       "      <td>0.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1779</td>\n",
       "      <td>0.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1781</td>\n",
       "      <td>0.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1782</td>\n",
       "      <td>0.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1783</td>\n",
       "      <td>0.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1786</td>\n",
       "      <td>0.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1787</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1788</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1789</td>\n",
       "      <td>0.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1791</td>\n",
       "      <td>1.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1793</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1794</td>\n",
       "      <td>0.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1796</td>\n",
       "      <td>0.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1797</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798</td>\n",
       "      <td>0.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1799</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1801</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1802</td>\n",
       "      <td>0.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1803</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1806</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1807</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>0.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>0.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1812</td>\n",
       "      <td>0.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1813</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1814</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>0.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>0.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>0.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1821</td>\n",
       "      <td>0.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1822</td>\n",
       "      <td>0.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1823</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1826</td>\n",
       "      <td>0.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1827</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1828</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1829</td>\n",
       "      <td>0.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1831</td>\n",
       "      <td>0.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>1.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>0.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1834</td>\n",
       "      <td>0.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>1.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1836</td>\n",
       "      <td>0.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1837</td>\n",
       "      <td>0.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>0.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>0.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>1.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1843</td>\n",
       "      <td>0.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1844</td>\n",
       "      <td>0.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>0.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1846</td>\n",
       "      <td>1.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1847</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1849</td>\n",
       "      <td>1.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1851</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1852</td>\n",
       "      <td>0.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1853</td>\n",
       "      <td>0.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1854</td>\n",
       "      <td>0.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1855</td>\n",
       "      <td>0.842800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1856</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1857</td>\n",
       "      <td>0.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1858</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1862</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1863</td>\n",
       "      <td>0.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1865</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1866</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1867</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1868</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1869</td>\n",
       "      <td>1.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871</td>\n",
       "      <td>0.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1873</td>\n",
       "      <td>0.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1874</td>\n",
       "      <td>0.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1876</td>\n",
       "      <td>0.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1877</td>\n",
       "      <td>0.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1878</td>\n",
       "      <td>1.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1879</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1882</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1883</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1884</td>\n",
       "      <td>0.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1885</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1886</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1887</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1889</td>\n",
       "      <td>0.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1891</td>\n",
       "      <td>0.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1892</td>\n",
       "      <td>0.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1893</td>\n",
       "      <td>0.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1894</td>\n",
       "      <td>1.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1895</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1896</td>\n",
       "      <td>0.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1897</td>\n",
       "      <td>0.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1898</td>\n",
       "      <td>0.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1901</td>\n",
       "      <td>0.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1903</td>\n",
       "      <td>0.787700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>0.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1906</td>\n",
       "      <td>0.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1907</td>\n",
       "      <td>0.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1908</td>\n",
       "      <td>0.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1909</td>\n",
       "      <td>0.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1911</td>\n",
       "      <td>0.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1912</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1913</td>\n",
       "      <td>0.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1914</td>\n",
       "      <td>0.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1915</td>\n",
       "      <td>1.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1916</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1917</td>\n",
       "      <td>0.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1918</td>\n",
       "      <td>0.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1919</td>\n",
       "      <td>0.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1921</td>\n",
       "      <td>0.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1922</td>\n",
       "      <td>0.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1923</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1924</td>\n",
       "      <td>1.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1926</td>\n",
       "      <td>1.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1927</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1928</td>\n",
       "      <td>0.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1929</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1931</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1932</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1933</td>\n",
       "      <td>0.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1934</td>\n",
       "      <td>0.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935</td>\n",
       "      <td>0.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1936</td>\n",
       "      <td>1.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1937</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1938</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1939</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1941</td>\n",
       "      <td>0.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1942</td>\n",
       "      <td>0.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943</td>\n",
       "      <td>1.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1945</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1946</td>\n",
       "      <td>0.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1947</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1948</td>\n",
       "      <td>1.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949</td>\n",
       "      <td>0.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>0.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1953</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1954</td>\n",
       "      <td>0.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>1.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>0.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1957</td>\n",
       "      <td>0.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1958</td>\n",
       "      <td>0.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1959</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1961</td>\n",
       "      <td>0.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1962</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1963</td>\n",
       "      <td>1.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1964</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1966</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1967</td>\n",
       "      <td>0.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>1.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1969</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1971</td>\n",
       "      <td>0.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1972</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1973</td>\n",
       "      <td>0.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1974</td>\n",
       "      <td>1.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1976</td>\n",
       "      <td>1.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1977</td>\n",
       "      <td>0.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1978</td>\n",
       "      <td>0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1979</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1981</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1982</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1983</td>\n",
       "      <td>0.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1987</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1988</td>\n",
       "      <td>0.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1989</td>\n",
       "      <td>0.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1991</td>\n",
       "      <td>0.645700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1993</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1994</td>\n",
       "      <td>0.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>1.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>1.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE bmm(16x256x256, 16x256x256)\n",
      "  triton_bmm_158 0.0175 ms 100.0%\n",
      "  triton_bmm_162 0.0205 ms 85.5%\n",
      "  triton_bmm_159 0.0225 ms 77.7%\n",
      "  triton_bmm_165 0.0245 ms 71.4%\n",
      "  triton_bmm_161 0.0246 ms 71.2%\n",
      "  triton_bmm_166 0.0246 ms 71.2%\n",
      "  triton_bmm_154 0.0266 ms 65.7%\n",
      "  triton_bmm_153 0.0276 ms 63.3%\n",
      "  triton_bmm_167 0.0297 ms 58.9%\n",
      "  triton_bmm_157 0.0299 ms 58.6%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.0745 seconds and 0.4624 seconds precompiling\n",
      "AUTOTUNE bmm(16x256x256, 16x256x256)\n",
      "  bmm 0.0184 ms 100.0%\n",
      "  triton_bmm_174 0.0256 ms 72.0%\n",
      "  triton_bmm_185 0.0266 ms 69.2%\n",
      "  triton_bmm_184 0.0287 ms 64.3%\n",
      "  triton_bmm_177 0.0297 ms 62.1%\n",
      "  triton_bmm_176 0.0307 ms 60.0%\n",
      "  triton_bmm_181 0.0307 ms 60.0%\n",
      "  triton_bmm_175 0.0317 ms 58.1%\n",
      "  triton_bmm_178 0.0317 ms 58.1%\n",
      "  triton_bmm_186 0.0317 ms 58.1%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.0796 seconds and 0.0008 seconds precompiling\n",
      "AUTOTUNE bmm(16x256x256, 16x256x256)\n",
      "  bmm 0.0215 ms 100.0%\n",
      "  triton_bmm_233 0.0276 ms 77.8%\n",
      "  triton_bmm_238 0.0276 ms 77.8%\n",
      "  triton_bmm_242 0.0277 ms 77.7%\n",
      "  triton_bmm_228 0.0289 ms 74.5%\n",
      "  triton_bmm_230 0.0306 ms 70.2%\n",
      "  triton_bmm_231 0.0307 ms 70.0%\n",
      "  triton_bmm_243 0.0307 ms 70.0%\n",
      "  triton_bmm_235 0.0328 ms 65.6%\n",
      "  triton_bmm_237 0.0348 ms 61.8%\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.1419 seconds and 0.0007 seconds precompiling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = \"../training/checkpoint_gemma2-2b-it/checkpoint-1000/\"\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_dir)\n",
    "else:\n",
    "    print(f\"Checkpoint directory {checkpoint_dir} not found, starting fresh training.\")\n",
    "    trainer_stats = trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c376880d-4af1-4071-849e-26c0e468a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.59 out of 15.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 26/26 [00:00<00:00, 49.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting gemma2 model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at seungjune/RBCadviser_GemmaFondo_gguf into bf16 GGUF format.\n",
      "The output location will be ./seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: RBCadviser_GemmaFondo_gguf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> BF16, shape = {2304, 256000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf: n_tensors = 288, total_size = 5.2G\n",
      "Writing: 100%|██████████| 5.23G/5.23G [00:22<00:00, 232Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3838 (f4d2b884)\n",
      "main: built with cc (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0 for x86_64-linux-gnu\n",
      "main: quantizing './seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf' to './seungjune/RBCadviser_GemmaFondo_gguf/unsloth.Q4_K_M.gguf' as Q4_K_M using 56 threads\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from ./seungjune/RBCadviser_GemmaFondo_gguf/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type bf16:  183 tensors\n",
      "[   1/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =   bf16, converting to q6_K .. size =  1125.00 MiB ->   461.43 MiB\n",
      "[   2/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   3/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[   4/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   5/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   6/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   7/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   8/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   9/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  10/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  11/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  12/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  13/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  14/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  15/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  16/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  17/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  18/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  19/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  20/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  21/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  22/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  23/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  24/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  25/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  26/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  27/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  28/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  29/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  30/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  31/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  32/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  33/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  34/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  35/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  36/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  37/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  38/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  39/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  40/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  41/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  42/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  43/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  44/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  45/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  46/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  47/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  48/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  49/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  50/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  51/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  52/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  53/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  54/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  55/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  56/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  57/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  58/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  59/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  60/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  61/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  62/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  63/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  64/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  65/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  66/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  67/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  68/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  69/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  70/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  71/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  72/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  73/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  74/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  75/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  76/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  77/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  78/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  79/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  80/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  81/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  82/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  83/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  84/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  85/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  86/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  87/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  88/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  89/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  90/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  91/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  92/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  93/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  94/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  95/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  96/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  97/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  98/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  99/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 100/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 101/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 102/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 103/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 104/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 105/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 106/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 107/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 108/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 109/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 110/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 111/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 112/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 113/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 114/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 115/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 116/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 117/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 118/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 119/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 120/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 121/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 122/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 123/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 124/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 125/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 126/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 127/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 128/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 129/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 130/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 131/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 132/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 133/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 134/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 135/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 136/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 137/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 138/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 139/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 140/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 141/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 142/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 143/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 144/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 145/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 146/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 147/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 148/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 149/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 150/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 151/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 152/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 153/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 154/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 155/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 156/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 157/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 158/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 159/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 160/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 161/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 162/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 163/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 164/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 165/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 166/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 167/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 168/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 169/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 170/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 171/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 172/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 173/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 174/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 175/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 176/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 177/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 178/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 179/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 180/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 181/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 182/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 183/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 184/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 185/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 186/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 187/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 188/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 189/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 190/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 191/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 192/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 193/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 194/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 195/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 196/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 197/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 198/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 199/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 200/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 201/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 202/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 203/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 204/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 205/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 206/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 207/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 208/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 209/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 210/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 211/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 212/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 213/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 214/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 215/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 216/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 217/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 218/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 219/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 220/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 221/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 222/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 223/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 224/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 225/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 226/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 227/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 228/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 229/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 230/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 231/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 232/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 233/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 234/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 235/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 236/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 237/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 238/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 239/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 240/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 241/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 242/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 243/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 244/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 245/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 246/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 247/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 248/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 249/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 250/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 251/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 252/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 253/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 254/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 255/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 256/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 257/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 258/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 259/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 260/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 261/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 262/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 263/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 264/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 265/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 266/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 267/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 268/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 269/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 270/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 271/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 273/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 274/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 275/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 276/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 277/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 278/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 279/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 280/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 281/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 282/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 283/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 284/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 285/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 286/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 287/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 288/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "llama_model_quantize_internal: model size  =  4986.92 MB\n",
      "llama_model_quantize_internal: quant size  =  1623.67 MB\n",
      "\n",
      "main: quantize time =  9107.43 ms\n",
      "main:    total time =  9107.43 ms\n",
      "Unsloth: Conversion completed! Output location: ./seungjune/RBCadviser_GemmaFondo_gguf/unsloth.Q4_K_M.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cf6e0807724f549316eb9745461dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.BF16.gguf:   0%|          | 0.00/5.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/seungjune/RBCadviser_GemmaFondo_gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58af3d957bbc4a4aa1b8635bbef66899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/seungjune/RBCadviser_GemmaFondo_gguf\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 레포지토리용 파라미터\n",
    "#base_model = \"unsloth/gemma-2-2b-bnb-4bit\" # pretrained 모델,\n",
    "huggingface_repo_gguf = \"seungjune/RBCadvisor_GemmaFondo_gguf\" # 업로드할 레포지토리\n",
    "\n",
    "quantization_method = \"q4_k_m\"   # \"q8_0\"  \"f16\" \"q8_0\" \"q4_k_m\" \"q5_k_m\"\n",
    "# Hub 에 GGUF 업로드\n",
    "model.push_to_hub_gguf(\n",
    "    huggingface_repo_gguf,\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_method,\n",
    "    token=hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83cf25-b418-4e29-a118-a8adfb2f0809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00a81822d2604c1e90bd233fc75de537": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "01520e584b884f938ee8213121e69586": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "04b8ec609ea145e4b1b2bd4a3006ac15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "04e7c9f8823a42429dc6d0bbe1c64fd7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "090e76fb6b654bfd806e42865fa2f013": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0fde0ff9e3294073b856e5367e939c9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1139e297e7de4e5abcc3a6c11425780b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "11d9619803e745eba6cb46edcb5d9ccb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e4a68c02ea84f319d32bcf905fcceaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9d8809fc21f94f8a96bd976901771a6a",
       "style": "IPY_MODEL_8eebbb6bcc6740f3b3e5b57cb28cfe87",
       "value": "Upload 2 LFS files: 100%"
      }
     },
     "2065f3f765394584a8883509e26a467a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2bac6016a5bc46109d81df262cdea6cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_eceb6e17301e4108866d5ab87dfa5f00",
       "style": "IPY_MODEL_9b60faf187d744268e033d4faf18863d",
       "value": " 4.24M/4.24M [00:01&lt;00:00, 31.2MB/s]"
      }
     },
     "2cc0b97331e84971861860ebcfc9079f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4f7885f8f9b94cc8a0c538d8247b94c4",
        "IPY_MODEL_457a915951d544a9adaee985b0307e0b",
        "IPY_MODEL_55ebd4d2ace8414593da21115cdf09c4"
       ],
       "layout": "IPY_MODEL_00a81822d2604c1e90bd233fc75de537"
      }
     },
     "2f81bf4b4a7c4300852c09f84cf2690b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "312d15de41c040879df68dd3288af787": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37c15f05a093448bbc1fa05f9c8445c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_04e7c9f8823a42429dc6d0bbe1c64fd7",
       "style": "IPY_MODEL_f5ac0a396ac6449ba2f0df3a9263912b",
       "value": "tokenizer.json: 100%"
      }
     },
     "3a209a61fbcc4c8d8700582c1c9d3e9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3cf53713d8ec4e57b59d40c5f14deeb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_784c05851bb74672b4660062c2d45af5",
       "max": 4241003,
       "style": "IPY_MODEL_090e76fb6b654bfd806e42865fa2f013",
       "value": 4241003
      }
     },
     "3cfbcd3598ad4f19a790888c9ed58a59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea0d9e95e7ed4f909dd5f9efb9486246",
        "IPY_MODEL_3cf53713d8ec4e57b59d40c5f14deeb1",
        "IPY_MODEL_d7a2bb471a30429097fe7f665c37e65f"
       ],
       "layout": "IPY_MODEL_ff938f9af6b947faa5133ef903fbea1a"
      }
     },
     "3e08a80c5ff44b30b921abcb4a7bbad9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e455595d97448628a4d00fab5b50565": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4317316626fa41b090437b2167c09c2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "457a915951d544a9adaee985b0307e0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_312d15de41c040879df68dd3288af787",
       "max": 2,
       "style": "IPY_MODEL_0fde0ff9e3294073b856e5367e939c9b",
       "value": 2
      }
     },
     "47cc5ad255e4477b9507a04861f93558": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4860f1eb0eff448bbc3f687d541c9be6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "488643ef60604424b0ab3c1c617fd93b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a64f2a71171d435b9986d10d4279aa91",
       "max": 17525357,
       "style": "IPY_MODEL_62bcbf8405164cca92b3845698007862",
       "value": 17525357
      }
     },
     "4b5e3eb2cda94acbbb1f2294fa2324cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4f7885f8f9b94cc8a0c538d8247b94c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6854ef1545d3491eae2d9264cf6e33d7",
       "style": "IPY_MODEL_c97b3a42fd6947dd82b1885c2849af32",
       "value": "Upload 2 LFS files: 100%"
      }
     },
     "51da022bd9434636aae8a3725a18b458": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_67bd0f9ff5ef48faa00ab983005e0b0c",
        "IPY_MODEL_488643ef60604424b0ab3c1c617fd93b",
        "IPY_MODEL_d567c91700134cfa828bf78799c531c8"
       ],
       "layout": "IPY_MODEL_a4f09c3aafb54c3ab4c145cc23588534"
      }
     },
     "55ebd4d2ace8414593da21115cdf09c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e0adcc082c814ec7b3eab9f5223bd25b",
       "style": "IPY_MODEL_5961aa694a5a4bb1bc6ac5f92d9d23fc",
       "value": " 2/2 [00:03&lt;00:00,  1.69s/it]"
      }
     },
     "58af3d957bbc4a4aa1b8635bbef66899": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7916dc7f04df499996388c81035823cc",
        "IPY_MODEL_d87aaa60116e45f594b4c286d7a70de2",
        "IPY_MODEL_6450d2ce5c6f4921ab0e8e22909d8851"
       ],
       "layout": "IPY_MODEL_68e3cf645361467d8e936bc9ffa6fd71"
      }
     },
     "5961aa694a5a4bb1bc6ac5f92d9d23fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59ee768fe7a443d18aa85a5028b3d841": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5ac9624f9f754e15b3ef93928fb3f21f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5baa3a13b88747d6bd262b1a3f49405d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5d592ef895d34373bf3da06a744d73bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5da2114a29374b01ae7abec68c0d53c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ac9624f9f754e15b3ef93928fb3f21f",
       "style": "IPY_MODEL_a4d17d130f504c6fa32ae76452d03326",
       "value": " 17.5M/17.5M [00:03&lt;00:00, 10.3MB/s]"
      }
     },
     "60bc49ff1a8a487eb4c9d93c9838339a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62bcbf8405164cca92b3845698007862": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6450d2ce5c6f4921ab0e8e22909d8851": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_04b8ec609ea145e4b1b2bd4a3006ac15",
       "style": "IPY_MODEL_db136bcd518e484fbb94ea0fca4a79d8",
       "value": " 1.71G/1.71G [01:20&lt;00:00, 25.2MB/s]"
      }
     },
     "67bd0f9ff5ef48faa00ab983005e0b0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_efceb17e4adc46e5b760eb1e6752bf98",
       "style": "IPY_MODEL_47cc5ad255e4477b9507a04861f93558",
       "value": "tokenizer.json: 100%"
      }
     },
     "6854ef1545d3491eae2d9264cf6e33d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68e3cf645361467d8e936bc9ffa6fd71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "71d296c2aafd4099806ced5283c6e670": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a20c3155d4e540a3bcf0301a1dd73db0",
       "style": "IPY_MODEL_da20b676d0804506b4d6ce0b93852e2a",
       "value": " 5.24G/5.24G [04:15&lt;00:00, 18.2MB/s]"
      }
     },
     "784c05851bb74672b4660062c2d45af5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7916dc7f04df499996388c81035823cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fd081b1999524335a5ef32de5a6cc491",
       "style": "IPY_MODEL_4317316626fa41b090437b2167c09c2f",
       "value": "unsloth.Q4_K_M.gguf: 100%"
      }
     },
     "79d2ffca900d42a6b1a29062c8b0abd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7cc7914034c94419a7c364aec0c3e5b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "88cf6e0807724f549316eb9745461dd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b7e04c7142c0497cadc6c360572b105c",
        "IPY_MODEL_90573987c41149a0a277444315cc4dd3",
        "IPY_MODEL_71d296c2aafd4099806ced5283c6e670"
       ],
       "layout": "IPY_MODEL_3e455595d97448628a4d00fab5b50565"
      }
     },
     "8d9d4c4d1eea4226af121b9803e26cd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4b5e3eb2cda94acbbb1f2294fa2324cf",
       "max": 2,
       "style": "IPY_MODEL_5d592ef895d34373bf3da06a744d73bd",
       "value": 2
      }
     },
     "8eebbb6bcc6740f3b3e5b57cb28cfe87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90573987c41149a0a277444315cc4dd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d843bd0803994627be08770db42719aa",
       "max": 5235213856,
       "style": "IPY_MODEL_60bc49ff1a8a487eb4c9d93c9838339a",
       "value": 5235213856
      }
     },
     "970b02cfe8644f079c9dbba64cfaba8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9761da90542e4c249430f61b39f7c3b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b60faf187d744268e033d4faf18863d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d8809fc21f94f8a96bd976901771a6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a20c3155d4e540a3bcf0301a1dd73db0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a38ee90be03f4ae9a709b07286146961": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1e4a68c02ea84f319d32bcf905fcceaa",
        "IPY_MODEL_8d9d4c4d1eea4226af121b9803e26cd5",
        "IPY_MODEL_db8f7df9ac2947faaaffe691bfe21584"
       ],
       "layout": "IPY_MODEL_3a209a61fbcc4c8d8700582c1c9d3e9c"
      }
     },
     "a4d17d130f504c6fa32ae76452d03326": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a4f09c3aafb54c3ab4c145cc23588534": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a64f2a71171d435b9986d10d4279aa91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a99f4dd6739b410f91830150067d2d48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c0fed4e117994f4a96db96a2030044ff",
       "style": "IPY_MODEL_4860f1eb0eff448bbc3f687d541c9be6",
       "value": "tokenizer.model: 100%"
      }
     },
     "acedaa9bd11a4dd794d85ab7c27d0329": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a99f4dd6739b410f91830150067d2d48",
        "IPY_MODEL_bb1823f1746346e097cbf88928d500ee",
        "IPY_MODEL_2bac6016a5bc46109d81df262cdea6cd"
       ],
       "layout": "IPY_MODEL_cc943802625141819978d2d7d10a2ef6"
      }
     },
     "b09f8c64b90f49a388d4434fe5fefb90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_37c15f05a093448bbc1fa05f9c8445c7",
        "IPY_MODEL_dbe8c876cb1e467094fc3fec3ff46640",
        "IPY_MODEL_5da2114a29374b01ae7abec68c0d53c1"
       ],
       "layout": "IPY_MODEL_da39e72ef0004bd88e774225cc4ddc6e"
      }
     },
     "b7e04c7142c0497cadc6c360572b105c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_59ee768fe7a443d18aa85a5028b3d841",
       "style": "IPY_MODEL_3e08a80c5ff44b30b921abcb4a7bbad9",
       "value": "unsloth.BF16.gguf: 100%"
      }
     },
     "bb1823f1746346e097cbf88928d500ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9761da90542e4c249430f61b39f7c3b4",
       "max": 4241003,
       "style": "IPY_MODEL_2f81bf4b4a7c4300852c09f84cf2690b",
       "value": 4241003
      }
     },
     "c0fed4e117994f4a96db96a2030044ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c5339ac66a8d4d06a37c685e76bbb1a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c97b3a42fd6947dd82b1885c2849af32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cc943802625141819978d2d7d10a2ef6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d567c91700134cfa828bf78799c531c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5baa3a13b88747d6bd262b1a3f49405d",
       "style": "IPY_MODEL_79d2ffca900d42a6b1a29062c8b0abd3",
       "value": " 17.5M/17.5M [00:03&lt;00:00, 9.45MB/s]"
      }
     },
     "d7a2bb471a30429097fe7f665c37e65f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7cc7914034c94419a7c364aec0c3e5b1",
       "style": "IPY_MODEL_db4bf64bc1794b338a04782252b674bb",
       "value": " 4.24M/4.24M [00:01&lt;00:00, 6.31MB/s]"
      }
     },
     "d843bd0803994627be08770db42719aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d87aaa60116e45f594b4c286d7a70de2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c5339ac66a8d4d06a37c685e76bbb1a5",
       "max": 1708582432,
       "style": "IPY_MODEL_f21d7c05463c4e5d81e221039b38d327",
       "value": 1708582432
      }
     },
     "da20b676d0804506b4d6ce0b93852e2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da39e72ef0004bd88e774225cc4ddc6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db136bcd518e484fbb94ea0fca4a79d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db4bf64bc1794b338a04782252b674bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db8f7df9ac2947faaaffe691bfe21584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1139e297e7de4e5abcc3a6c11425780b",
       "style": "IPY_MODEL_11d9619803e745eba6cb46edcb5d9ccb",
       "value": " 2/2 [00:03&lt;00:00,  1.78s/it]"
      }
     },
     "dbe8c876cb1e467094fc3fec3ff46640": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2065f3f765394584a8883509e26a467a",
       "max": 17525357,
       "style": "IPY_MODEL_970b02cfe8644f079c9dbba64cfaba8c",
       "value": 17525357
      }
     },
     "e0adcc082c814ec7b3eab9f5223bd25b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ea0d9e95e7ed4f909dd5f9efb9486246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_01520e584b884f938ee8213121e69586",
       "style": "IPY_MODEL_f49958585c5a41b3a229cc38f9f8ae46",
       "value": "tokenizer.model: 100%"
      }
     },
     "eceb6e17301e4108866d5ab87dfa5f00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "efceb17e4adc46e5b760eb1e6752bf98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f21d7c05463c4e5d81e221039b38d327": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f49958585c5a41b3a229cc38f9f8ae46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f5ac0a396ac6449ba2f0df3a9263912b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fd081b1999524335a5ef32de5a6cc491": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ff938f9af6b947faa5133ef903fbea1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
